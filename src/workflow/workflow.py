"""LangGraph ì›Œí¬í”Œë¡œìš° ëª¨ë“ˆ - queryë§Œ ì…ë ¥ë°›ìŒ"""

import logging
from typing import TypedDict, Literal, Optional
from langgraph.graph import StateGraph, END
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from src.chains import (
    create_intent_classifier,
    create_recommendation_chain,
    create_summary_chain,
    create_quiz_chain,
)
from src.chains.common import get_llm

logger = logging.getLogger(__name__)


# ìƒíƒœ ì •ì˜ - queryë§Œ í•„ìˆ˜ ì…ë ¥
class _WorkflowStateRequired(TypedDict):
    """í•„ìˆ˜ í•„ë“œ"""
    query: str  # ì‚¬ìš©ì ì…ë ¥ (ìœ ì¼í•œ ì…ë ¥)


class WorkflowState(_WorkflowStateRequired, total=False):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ - queryë§Œ í•„ìˆ˜, ë‚˜ë¨¸ì§€ëŠ” ìë™ ì´ˆê¸°í™”"""
    intent: str  # ì˜ë„ ë¶„ë¥˜ ê²°ê³¼
    recommendation_result: Optional[str]  # ì¶”ì²œ ê²°ê³¼
    final_result: str  # ìµœì¢… ê²°ê³¼
    need_summary: bool  # ìš”ì•½ í•„ìš” ì—¬ë¶€ (LLMì´ íŒë‹¨)


def create_workflow_app(
    vectorstore,
    llm_config: Optional[dict] = None,
):
    """
    LangGraph ì›Œí¬í”Œë¡œìš° ì•± ìƒì„±

    Args:
        vectorstore: RAG ë²¡í„°ìŠ¤í† ì–´
        llm_config: LLM ì„¤ì • ë”•ì…”ë„ˆë¦¬
            {
                "model": "gpt-4o-mini",
                "temperature": 0.7,
                "max_tokens": None,
                ...
            }

    Returns:
        ì»´íŒŒì¼ëœ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
    """
    if llm_config is None:
        llm_config = {}

    # ê¸°ë³¸ê°’ ì„¤ì •
    default_config = {
        "model": "gpt-4o-mini",
        "temperature": 0.7,
        "max_tokens": None,
    }
    default_config.update(llm_config)

    # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    from src.rag.retriever import create_retriever
    retriever = create_retriever(vectorstore, retriever_type="basic", k=4)

    # ì²´ì¸ ìƒì„±
    logger.info("ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘...")
    intent_classifier = create_intent_classifier(
        model=default_config["model"],
        temperature=0.3,  # ì˜ë„ë¶„ë¥˜ëŠ” ë‚®ì€ ì˜¨ë„ ì‚¬ìš©
        max_tokens=default_config["max_tokens"],
    )

    recommendation_chain = create_recommendation_chain(
        retriever,
        model=default_config["model"],
        temperature=default_config["temperature"],
        max_tokens=default_config["max_tokens"],
    )

    summary_chain = create_summary_chain(
        retriever,
        model=default_config["model"],
        temperature=default_config["temperature"],
        max_tokens=default_config["max_tokens"],
    )

    quiz_chain = create_quiz_chain(
        retriever,
        model=default_config["model"],
        temperature=default_config["temperature"],
        max_tokens=default_config["max_tokens"],
    )

    # ë…¸ë“œ í•¨ìˆ˜ ì •ì˜

    def classify_intent(state: WorkflowState) -> WorkflowState:
        """ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        query = state["query"]
        intent = intent_classifier.invoke({"query": query}).strip().lower()
        logger.info(f"ğŸ¯ ì˜ë„ ë¶„ë¥˜: {intent}")
        return {
            **state,
            "intent": intent,
            "need_summary": False,  # ì´ˆê¸°ê°’
        }

    def extract_dish_name(query: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ìš”ë¦¬ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        llm = get_llm(model=default_config["model"], temperature=0.3)
        extract_dish_prompt = ChatPromptTemplate.from_messages([
            ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ìš”ë¦¬ëª…ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ë¬¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”."),
            ("user", "{query}")
        ])
        dish_extractor = extract_dish_prompt | llm | StrOutputParser()
        dish_name = dish_extractor.invoke({"query": query})
        return dish_name.strip()

    def run_recommendation(state: WorkflowState) -> WorkflowState:
        """ì¶”ì²œ ì²´ì¸ì„ ì‹¤í–‰í•˜ê³ , LLMìœ¼ë¡œ ìš”ì•½ í•„ìš”ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤."""
        logger.info("ğŸ³ ì¶”ì²œ ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

        # ìš”ë¦¬ëª… ì¶”ì¶œ
        dish_name = extract_dish_name(state["query"])
        logger.info(f"ì¶”ì¶œëœ ìš”ë¦¬ëª…: {dish_name}")

        # ì¶”ì²œ ì²´ì¸ ì‹¤í–‰
        result = recommendation_chain({"dish_name": dish_name})
        logger.info("âœ… ì¶”ì²œ ì²´ì¸ ì™„ë£Œ")

        # LLMìœ¼ë¡œ ìš”ì•½ í•„ìš”ì„± íŒë‹¨
        logger.info("ğŸ¤” ìš”ì•½ í•„ìš”ì„± íŒë‹¨ ì¤‘...")
        llm = get_llm(model=default_config["model"], temperature=0.3)
        summary_decision_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ì²œ í›„ ì¡°ë¦¬ë²•ê³¼ ì£¼ì˜ì‚¬í•­ ìš”ì•½ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”:
- "yes": ìš”ì•½ì´ í•„ìš”í•œ ê²½ìš° (ì‚¬ìš©ìê°€ ì¡°ë¦¬ë²•, ì£¼ì˜ì‚¬í•­, íŒ ë“±ì„ ìš”ì²­í•œ ê²½ìš°)
- "no": ìš”ì•½ì´ ë¶ˆí•„ìš”í•œ ê²½ìš° (ë‹¨ìˆœ ì¬ë£Œ ëŒ€ì²´ ì¶”ì²œë§Œ ì›í•˜ëŠ” ê²½ìš°)

íŒë‹¨ ê¸°ì¤€:
- "ë§Œë“œëŠ” ë²•", "ì¡°ë¦¬ë²•", "ì–´ë–»ê²Œ", "ë°©ë²•", "ì£¼ì˜", "íŒ", "ì•Œë ¤ì¤„ë˜" ë“±ì˜ í‚¤ì›Œë“œ í¬í•¨ â†’ yes
- "ì¶”ì²œ", "ëŒ€ì²´", "ë­", "ë­˜", "ë­ê°€", "ê°€ëŠ¥í•œ", "í•  ìˆ˜" ë“±ë§Œ í¬í•¨ â†’ no"""),
            ("user", "{query}")
        ])

        summary_decision_chain = summary_decision_prompt | llm | StrOutputParser()
        decision = summary_decision_chain.invoke({"query": state["query"]}).strip().lower()

        need_summary = "yes" in decision
        logger.info(f"ìš”ì•½ í•„ìš”ì„±: {'í•„ìš”' if need_summary else 'ë¶ˆí•„ìš”'} (íŒë‹¨: {decision})")

        return {
            **state,
            "recommendation_result": result,
            "final_result": result,
            "need_summary": need_summary,
        }

    def run_summary(state: WorkflowState) -> WorkflowState:
        """ìš”ì•½ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("ğŸ“ ìš”ì•½ ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

        result = summary_chain({"topic": state["query"]})
        logger.info("âœ… ìš”ì•½ ì²´ì¸ ì™„ë£Œ")

        # ì¶”ì²œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê²°í•©, ì—†ìœ¼ë©´ ê·¸ëƒ¥ ìš”ì•½ ê²°ê³¼ë§Œ ë°˜í™˜
        if state.get("recommendation_result"):
            final_result = f"{state['recommendation_result']}\n\n{'='*70}\n\n## ì¶”ê°€ ì •ë³´\n\n{result}"
        else:
            final_result = result

        return {**state, "final_result": final_result}

    def run_quiz(state: WorkflowState) -> WorkflowState:
        """ë¬¸ì œ ìƒì„± ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("â“ ë¬¸ì œ ìƒì„± ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

        result = quiz_chain({"topic": state["query"]})
        logger.info("âœ… ë¬¸ì œ ìƒì„± ë…¸ë“œ ì™„ë£Œ")

        return {**state, "final_result": result}

    # ë¼ìš°í„° í•¨ìˆ˜
    def route_after_recommendation(state: WorkflowState) -> Literal["summary", "end"]:
        """ì¶”ì²œ í›„ ìš”ì•½ í•„ìš”ì„±ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤ (LangGraph conditional_edges)"""
        if state.get("need_summary", False):
            logger.info("â†’ Summary ë…¸ë“œë¡œ ë¼ìš°íŒ…")
            return "summary"
        else:
            logger.info("â†’ ì¢…ë£Œ")
            return "end"

    def route_intent(state: WorkflowState) -> Literal["recommendation", "summary", "quiz"]:
        """ì˜ë„ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤"""
        intent = state["intent"]

        if "recommendation" in intent:
            logger.info("â†’ Recommendation ë…¸ë“œë¡œ ë¼ìš°íŒ…")
            return "recommendation"
        elif "quiz" in intent:
            logger.info("â†’ Quiz ë…¸ë“œë¡œ ë¼ìš°íŒ…")
            return "quiz"
        else:
            logger.info("â†’ Summary ë…¸ë“œë¡œ ë¼ìš°íŒ…")
            return "summary"

    # LangGraph êµ¬ì„±
    workflow = StateGraph(WorkflowState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("classifier", classify_intent)
    workflow.add_node("recommendation", run_recommendation)
    workflow.add_node("summary", run_summary)
    workflow.add_node("quiz", run_quiz)

    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("classifier")

    # ì˜ë„ ë¶„ë¥˜ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "classifier",
        route_intent,
        {
            "recommendation": "recommendation",
            "summary": "summary",
            "quiz": "quiz",
        }
    )

    # Recommendation í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… (need_summary í”Œë˜ê·¸ ê¸°ë°˜)
    workflow.add_conditional_edges(
        "recommendation",
        route_after_recommendation,
        {
            "summary": "summary",
            "end": END,
        }
    )

    # ê° ì²´ì¸ ë…¸ë“œì—ì„œ ì¢…ë£Œë¡œ ì—°ê²°
    workflow.add_edge("summary", END)
    workflow.add_edge("quiz", END)

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile()
    logger.info("âœ… ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ")

    return app
